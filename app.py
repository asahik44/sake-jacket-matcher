import streamlit as st
import streamlit.components.v1 as components
import pickle
import numpy as np
import torch
from sentence_transformers import SentenceTransformer, util
from transformers import BertTokenizer, BertForSequenceClassification
import torch.nn.functional as F
import os
import traceback

# ==========================================
# â˜…è¨­å®šã‚¨ãƒªã‚¢
# ==========================================
DEBUG_MODE = True  
APP_TITLE = "Sake Jacket Matcher"

GENRE_ORDER = [
    "ãƒ“ãƒ¼ãƒ«", "æµ·å¤–ãƒ“ãƒ¼ãƒ«", "åœ°ãƒ“ãƒ¼ãƒ«ãƒ»ã‚¯ãƒ©ãƒ•ãƒˆãƒ“ãƒ¼ãƒ«",
    "ã‚¦ã‚¤ã‚¹ã‚­ãƒ¼", "ãƒ¯ã‚¤ãƒ³", "èµ¤ãƒ¯ã‚¤ãƒ³", "ç™½ãƒ¯ã‚¤ãƒ³", "ã‚¹ãƒ‘ãƒ¼ã‚¯ãƒªãƒ³ã‚°ãƒ¯ã‚¤ãƒ³", "ã‚·ãƒ£ãƒ³ãƒ‘ãƒ³",
    "æ—¥æœ¬é…’", "ç„¼é…", "èŠ‹ç„¼é…", "éº¦ç„¼é…", "ç±³ç„¼é…",
    "ã‚µãƒ¯ãƒ¼ã®ç´ ãƒ»å‰²æ", "ãƒªã‚­ãƒ¥ãƒ¼ãƒ«", "ã‚¸ãƒ³ãƒ»ã‚¯ãƒ©ãƒ•ãƒˆã‚¸ãƒ³", "æ¢…é…’",
    "ãƒãƒ³ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«"
]

st.set_page_config(page_title=APP_TITLE, layout="wide")

def inject_ga():
    try:
        if "GA_ID" in st.secrets:
            GA_ID = st.secrets["GA_ID"]
            ga_code = f"""<script async src="https://www.googletagmanager.com/gtag/js?id={GA_ID}"></script><script>window.dataLayer = window.dataLayer || [];function gtag(){{dataLayer.push(arguments);}}gtag('js', new Date());gtag('config', '{GA_ID}');</script>"""
            components.html(ga_code, height=0)
    except Exception:
        pass

inject_ga()

st.markdown("""
<style>
    header {visibility: visible !important; background-color: transparent !important;}
    footer {visibility: hidden !important; display: none !important;}
    div[data-testid="stDecoration"] {visibility: hidden; display: none;}
    div[class*="viewerBadge"] {visibility: hidden !important; display: none !important;}
    .viewerBadge_container__1QSob {display: none !important;}
    div[data-testid="stImage"] img { height: 200px; object-fit: contain; width: 100%; }
</style>
""", unsafe_allow_html=True)

# --- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ (â˜…ã“ã“ã‚’é«˜é€ŸåŒ–ï¼) ---
@st.cache_resource
def load_all_models():
    try:
        with open('sake_database.pkl', 'rb') as f:
            db_data = pickle.load(f)
    except FileNotFoundError:
        st.error("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹(sake_database.pkl)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    try:
        clip_model = SentenceTransformer('sentence-transformers/clip-ViT-B-32-multilingual-v1')
        
        # â˜…â˜…â˜… é«˜é€ŸåŒ–ãƒã‚¤ãƒ³ãƒˆ â˜…â˜…â˜…
        # ã“ã“ã§æœ€åˆã«ã€ŒPyTorchã®Tensorï¼ˆè¨ˆç®—ã—ã‚„ã™ã„å½¢ï¼‰ã€ã«å¤‰æ›ã—ã¦ãŠãï¼
        # ã“ã‚Œã§æ¤œç´¢ã®ãŸã³ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒãªããªã‚Šã¾ã™ã€‚
        raw_vectors = np.concatenate([item['vector'] for item in db_data], axis=0)
        all_vectors_tensor = torch.tensor(raw_vectors).float().cpu()
        
    except Exception as e:
        st.error(f"CLIPãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    
    raw_genres = list(set([item.get('genre', 'ãã®ä»–') for item in db_data]))
    sorted_genres = sorted(raw_genres, key=lambda x: GENRE_ORDER.index(x) if x in GENRE_ORDER else 999)

    intent_tk, intent_md, genre_tk, genre_md = None, None, None, None
    has_logic_model = False
    try:
        if os.path.exists("./my_intent_model") and os.path.exists("./my_genre_model"):
            intent_tk = BertTokenizer.from_pretrained("./my_intent_model")
            intent_md = BertForSequenceClassification.from_pretrained("./my_intent_model")
            genre_tk = BertTokenizer.from_pretrained("./my_genre_model")
            genre_md = BertForSequenceClassification.from_pretrained("./my_genre_model")
            has_logic_model = True
    except Exception:
        pass 

    return {
        "db": db_data,
        "clip": clip_model,
        "vectors": all_vectors_tensor, # â˜…Tensorã‚’æ¸¡ã™
        "genres": sorted_genres,
        "intent_tk": intent_tk, 
        "intent_md": intent_md, 
        "genre_tk": genre_tk, 
        "genre_md": genre_md, 
        "has_logic_model": has_logic_model
    }

models = load_all_models()
if not models: st.stop()

# --- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é–¢æ•° ---
def predict_intent(text):
    if not models["has_logic_model"]: return False, 0.0
    inputs = models["intent_tk"](text, return_tensors="pt", padding=True, truncation=True, max_length=64)
    with torch.no_grad(): outputs = models["intent_md"](**inputs)
    probs = F.softmax(outputs.logits, dim=-1)
    return probs[0][1].item() > 0.5, probs[0][1].item()

def predict_genre_probs(text):
    if not models["has_logic_model"]: return {}
    inputs = models["genre_tk"](text, return_tensors="pt", padding=True, truncation=True, max_length=64)
    with torch.no_grad(): outputs = models["genre_md"](**inputs)
    probs = F.softmax(outputs.logits, dim=-1)[0]
    return {models["genre_md"].config.id2label[i]: prob.item() for i, prob in enumerate(probs)}

# MMRãƒ­ã‚¸ãƒƒã‚¯ (è»½é‡åŒ–)
def mmr_sort(query_vec, candidate_vectors_tensor, candidate_items, top_k=12, diversity=0.4):
    try:
        # ã‚¯ã‚¨ãƒªã ã‘TensoråŒ–ï¼ˆå€™è£œã¯ã™ã§ã«Tensorãªã®ã§å¤‰æ›ä¸è¦ï¼‰
        query_tensor = torch.tensor(query_vec).float().cpu()
        if query_tensor.dim() == 1: query_tensor = query_tensor.unsqueeze(0)
        
        # â˜… è¨ˆç®— (candidate_vectors_tensor ã¯ã™ã§ã«Tensor)
        sims_to_query = util.cos_sim(query_tensor, candidate_vectors_tensor)[0]
        
        selected_indices = []
        candidate_indices = list(range(len(candidate_items)))
        
        if len(candidate_items) <= top_k:
            sorted_indices = torch.argsort(sims_to_query, descending=True).tolist()
            return [candidate_items[i] for i in sorted_indices], [sims_to_query[i].item() for i in sorted_indices]

        for _ in range(min(len(candidate_items), top_k)):
            best_mmr_score = -float('inf')
            best_idx = -1
            for idx in candidate_indices:
                similarity_to_query = sims_to_query[idx].item()
                if selected_indices:
                    # ã“ã“ã‚‚TensoråŒå£«ã®è¨ˆç®—ãªã®ã§é«˜é€Ÿ
                    selected_vecs = candidate_vectors_tensor[selected_indices]
                    current_vec = candidate_vectors_tensor[idx].unsqueeze(0)
                    sim_to_selected = util.cos_sim(current_vec, selected_vecs)
                    max_similarity_to_selected = torch.max(sim_to_selected).item()
                else:
                    max_similarity_to_selected = 0
                
                mmr_score = (1 - diversity) * similarity_to_query - diversity * max_similarity_to_selected
                if mmr_score > best_mmr_score:
                    best_mmr_score = mmr_score
                    best_idx = idx
            selected_indices.append(best_idx)
            candidate_indices.remove(best_idx)
            
        return [candidate_items[i] for i in selected_indices], [sims_to_query[i].item() for i in selected_indices]
    except Exception as e:
        st.error(f"MMR Error: {e}")
        return [], []

# --- æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³æœ¬ä½“ (é«˜é€ŸåŒ–ç‰ˆ) ---
def search_engine(original_query, selected_genres, min_p, max_p, mode="visual", logic_mode="A"):
    ai_message = ""
    search_genres = []
    
    # å®Ÿæ³ã¯DEBUG_MODEã®æ™‚ã ã‘æ§ãˆã‚ã«å‡ºã™
    if DEBUG_MODE: st.write("ğŸƒâ€â™‚ï¸ [STEP 1] æ¤œç´¢é–‹å§‹")
    
    try:
        if mode == "visual" and ("C" in logic_mode or "D" in logic_mode):
            query_for_clip = f"ã€Œ{original_query}ã€ã¨ã„ã†é›°å›²æ°—ã®ãŠé…’ã®ãƒœãƒˆãƒ«ãƒ‡ã‚¶ã‚¤ãƒ³ã€‚ Package design of sake bottle with the vibe of {original_query}."
        else:
            query_for_clip = original_query

        if selected_genres:
            search_genres = selected_genres
        elif mode == "logic" and models["has_logic_model"]:
            # (çœç•¥) Logicéƒ¨åˆ†...
            target_genres = []
            for broad_key, children in BROAD_CATEGORIES.items():
                if broad_key in original_query: target_genres.extend(children)
            for g in models["genres"]:
                if g in original_query and g not in target_genres: target_genres.append(g)
            if target_genres:
                search_genres = list(set(target_genres))
                ai_message = "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ã‚¸ãƒ£ãƒ³ãƒ«ã‚’çµã‚Šè¾¼ã¿ã¾ã—ãŸ"
            else:
                is_nonal, nonal_conf = predict_intent(original_query)
                if is_nonal:
                    search_genres = ["ãƒãƒ³ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«"]
                    ai_message = "ãƒãƒ³ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«å•†å“ã‹ã‚‰æ¢ã—ã¾ã™"
                else:
                    genre_probs = predict_genre_probs(original_query)
                    sorted_genres = sorted(genre_probs.items(), key=lambda x: x[1], reverse=True)
                    candidates = [sorted_genres[0][0]]
                    for g, p in sorted_genres[1:]:
                        if p > 0.15: candidates.append(g)
                    search_genres = candidates
                    ai_message = f"AIæ¨è«–: {search_genres[0]} ãªã©ãŒåˆã„ãã†ã§ã™"
        elif mode == "visual" or not models["has_logic_model"]:
            search_genres = [] 
            ai_message = ""

        # ã‚¯ã‚¨ãƒªã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        query_vec = models["clip"].encode(query_for_clip, convert_to_tensor=True).float().cpu().numpy()
        if query_vec.ndim == 1: query_vec = query_vec[None, :] 
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        valid_indices = []
        for i, item in enumerate(models["db"]):
            if search_genres and item.get('genre') not in search_genres: continue
            if not (min_p <= item['price'] <= max_p): continue
            valid_indices.append(i)
            
        if not valid_indices: 
            return [], ai_message
        
        # â˜…â˜…â˜… é«˜é€ŸåŒ–ãƒã‚¤ãƒ³ãƒˆ â˜…â˜…â˜…
        # æ¯å› torch.tensor() ã™ã‚‹ã®ã‚’ã‚„ã‚ã¾ã—ãŸã€‚
        # ã™ã§ã«Tensorã«ãªã£ã¦ã„ã‚‹ models["vectors"] ã‹ã‚‰ã‚¹ãƒ©ã‚¤ã‚¹ã™ã‚‹ã ã‘ã€‚ä¸€ç¬ã§ã™ã€‚
        target_vectors_tensor = models["vectors"][valid_indices]
        candidate_items = [models["db"][i] for i in valid_indices]

        if DEBUG_MODE: st.write(f"ğŸƒâ€â™‚ï¸ [STEP 4] è¨ˆç®—é–‹å§‹ Mode: {logic_mode}")

        if mode == "visual" and ("B" in logic_mode or "D" in logic_mode):
            results, raw_scores = mmr_sort(query_vec, target_vectors_tensor, candidate_items, top_k=12, diversity=0.4)
        else:
            # Baseline (é«˜é€ŸåŒ–æ¸ˆã¿)
            q_tensor = torch.tensor(query_vec).float().cpu()
            
            # target_vectors_tensor ã¯ã™ã§ã«Tensorãªã®ã§å¤‰æ›ä¸è¦ï¼
            scores = util.cos_sim(q_tensor, target_vectors_tensor)
            scores = scores[0] 
            
            sorted_args = torch.argsort(scores, descending=True)
            
            results = []
            raw_scores = []
            for i in range(min(12, len(sorted_args))):
                idx = sorted_args[i].item()
                results.append(candidate_items[idx])
                raw_scores.append(scores[idx].item())

        if DEBUG_MODE: st.write("ğŸƒâ€â™‚ï¸ [STEP 5] å®Œäº†")

        final_results = []
        for item, raw_score in zip(results, raw_scores):
            display_score = min(raw_score * 5.0, 0.99)
            item['match_score'] = display_score
            final_results.append(item)
            
        return final_results, ai_message

    except Exception as e:
        st.error(f"ğŸš¨ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        st.code(traceback.format_exc())
        return [], "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼"

# --- UIæ§‹ç¯‰ ---
st.title(f"ğŸ¾ {APP_TITLE}")
st.sidebar.header("Search Mode")

if models["has_logic_model"]:
    mode_options = ("ã‚¸ãƒ£ã‚±è²·ã„ (æ„Ÿæ€§)", "AIã‚½ãƒ ãƒªã‚¨ (çŸ¥è­˜)")
else:
    mode_options = ("ã‚¸ãƒ£ã‚±è²·ã„ (æ„Ÿæ€§)",) 
mode_select = st.sidebar.radio("æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰", mode_options, index=0)
mode_key = "visual" if "ã‚¸ãƒ£ã‚±è²·ã„" in mode_select else "logic"

st.sidebar.divider()
st.sidebar.header("Filters")
user_genres = st.sidebar.multiselect("ã‚¸ãƒ£ãƒ³ãƒ«å›ºå®š", options=models["genres"])
price_range = st.sidebar.slider("ä¾¡æ ¼å¸¯", 0, 30000, (0, 30000), 500, format="Â¥%d")

st.sidebar.divider()
st.sidebar.markdown("### ğŸ§ª é–‹ç™ºè€…ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
logic_mode = st.sidebar.selectbox("æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¤œè¨¼", ["A: é€šå¸¸ (Baseline)", "B: MMR (å¤šæ§˜æ€§é‡è¦–)", "C: Prompt (è¨€è‘‰ã‚’è£œæ­£)", "D: MMR + Prompt (æœ€å¼·?)"], index=0)
if DEBUG_MODE: st.sidebar.warning("ğŸ”§ ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ ON")

col1, col2 = st.columns([3, 1], vertical_alignment="bottom")
with col1:
    placeholder = "ä¾‹ï¼šã‚µã‚¤ãƒãƒ¼ãƒ‘ãƒ³ã‚¯ãªå¤œ..." 
    query = st.text_input("ã©ã‚“ãªé›°å›²æ°—ã®ãŠé…’ãŒã„ã„ï¼Ÿ", placeholder=placeholder).strip()
with col2:
    search_btn = st.button("Digã‚‹", type="primary", use_container_width=True)

if query or search_btn:
    st.divider()
    results, message = search_engine(query, user_genres, price_range[0], price_range[1], mode=mode_key, logic_mode=logic_mode)
    
    if message: st.caption(message)
    
    if results:
        cols = st.columns(3)
        for i, item in enumerate(results):
            with cols[i % 3]:
                with st.container(height=450, border=True): 
                    if item.get('image_url'): st.image(item['image_url'], use_container_width=True)
                    else: st.text("No Image")
                    
                    # Scoreè¡¨ç¤º (Visualãƒ¢ãƒ¼ãƒ‰ã®ã¿)
                    if mode_key == "visual":
                        st.progress(item['match_score'], text=f"Match: {int(item['match_score']*100)}%")
                    
                    st.write(f"**{item['name'][:30]}**")
                    st.link_button("æ¥½å¤©ã§è¦‹ã‚‹ â¤", item['url'], use_container_width=True)
    else:
        if message != "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼":
            st.warning("âš ï¸ çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ (Not Found)")