import streamlit as st
import streamlit.components.v1 as components
import pickle
import numpy as np
import torch
from sentence_transformers import SentenceTransformer, util
from transformers import BertTokenizer, BertForSequenceClassification
import torch.nn.functional as F
import os 

# ==========================================
# â˜…Google Analyticsè¨­å®š
# ==========================================
def inject_ga():
    if "GA_ID" in st.secrets:
        GA_ID = st.secrets["GA_ID"]
        ga_code = f"""
        <script async src="https://www.googletagmanager.com/gtag/js?id={GA_ID}"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){{dataLayer.push(arguments);}}
            gtag('js', new Date());
            gtag('config', '{GA_ID}');
        </script>
        """
        components.html(ga_code, height=0)

# ==========================================
# â˜…è¨­å®šã‚¨ãƒªã‚¢
# ==========================================
DEBUG_MODE = False  # Trueã«ã™ã‚‹ã¨è£å´ã®ã‚¹ã‚³ã‚¢ãªã©ãŒè¦‹ãˆã¾ã™
APP_TITLE = "Sake Jacket Matcher"

GENRE_ORDER = [
    "ãƒ“ãƒ¼ãƒ«", "æµ·å¤–ãƒ“ãƒ¼ãƒ«", "åœ°ãƒ“ãƒ¼ãƒ«ãƒ»ã‚¯ãƒ©ãƒ•ãƒˆãƒ“ãƒ¼ãƒ«",
    "ã‚¦ã‚¤ã‚¹ã‚­ãƒ¼", "ãƒ¯ã‚¤ãƒ³", "èµ¤ãƒ¯ã‚¤ãƒ³", "ç™½ãƒ¯ã‚¤ãƒ³", "ã‚¹ãƒ‘ãƒ¼ã‚¯ãƒªãƒ³ã‚°ãƒ¯ã‚¤ãƒ³", "ã‚·ãƒ£ãƒ³ãƒ‘ãƒ³",
    "æ—¥æœ¬é…’", "ç„¼é…", "èŠ‹ç„¼é…", "éº¦ç„¼é…", "ç±³ç„¼é…",
    "ã‚µãƒ¯ãƒ¼ã®ç´ ãƒ»å‰²æ", "ãƒªã‚­ãƒ¥ãƒ¼ãƒ«", "ã‚¸ãƒ³ãƒ»ã‚¯ãƒ©ãƒ•ãƒˆã‚¸ãƒ³", "æ¢…é…’",
    "ãƒãƒ³ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«"
]

# ==========================================
# ã‚¢ãƒ—ãƒªè¨­å®š
# ==========================================
st.set_page_config(page_title=APP_TITLE, layout="wide")
inject_ga()

# â˜…ãƒ•ã‚£ãƒ«ã‚¿ãƒœã‚¿ãƒ³ã‚’æ®‹ã—ã¤ã¤ç™½æ ã‚’æ¶ˆã™CSS
st.markdown("""
<style>
    /* 1. ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆä¸Šã®ãƒãƒ¼ï¼‰ã¯ã€Œè¡¨ç¤ºã€ã•ã›ã‚‹ï¼ */
    /* ã“ã‚ŒãŒãªã„ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒœã‚¿ãƒ³ï¼ˆï¼ï¼‰ãŒæ¶ˆãˆã¦ã—ã¾ã„ã¾ã™ */
    header {
        visibility: visible !important;
        background-color: transparent !important;
    }
    
    /* 2. é€šå¸¸ã®ãƒ•ãƒƒã‚¿ãƒ¼ã‚’æ¶ˆã™ */
    footer {
        visibility: hidden !important;
        display: none !important;
    }
    
    /* 3. è™¹è‰²ã®ç·šã ã‘æ¶ˆã™ */
    div[data-testid="stDecoration"] {
        visibility: hidden;
        display: none;
    }

    /* 4. Streamlit Cloudç‰¹æœ‰ã®ã€Œç™½æ ï¼ˆViewerBadgeï¼‰ã€ã‚’å¼·åˆ¶çš„ã«æ¶ˆã™ */
    div[class*="viewerBadge"] {
        visibility: hidden !important;
        display: none !important;
    }
    .viewerBadge_container__1QSob {
        display: none !important;
    }

    /* ç”»åƒã‚µã‚¤ã‚ºã®èª¿æ•´ */
    div[data-testid="stImage"] img { height: 200px; object-fit: contain; width: 100%; }
</style>
""", unsafe_allow_html=True)

# --- å®šç¾© ---
BROAD_CATEGORIES = {
    "æ´‹é…’": ["ã‚¦ã‚¤ã‚¹ã‚­ãƒ¼", "ãƒ–ãƒ©ãƒ³ãƒ‡ãƒ¼", "ã‚¸ãƒ³ãƒ»ã‚¯ãƒ©ãƒ•ãƒˆã‚¸ãƒ³", "ã‚¦ã‚©ãƒƒã‚«", "ãƒ©ãƒ ", "ãƒ†ã‚­ãƒ¼ãƒ©", "ãƒªã‚­ãƒ¥ãƒ¼ãƒ«", "èµ¤ãƒ¯ã‚¤ãƒ³", "ç™½ãƒ¯ã‚¤ãƒ³"],
    "ç„¼é…": ["èŠ‹ç„¼é…", "éº¦ç„¼é…", "ç±³ç„¼é…", "é»’ç³–ç„¼é…", "æ³¡ç››"],
    "ã‚¦ã‚£ã‚¹ã‚­ãƒ¼": ["ã‚¦ã‚¤ã‚¹ã‚­ãƒ¼"], "ã‚¦ãƒ°ã‚¹ã‚­ãƒ¼": ["ã‚¦ã‚¤ã‚¹ã‚­ãƒ¼"], "WHISKY": ["ã‚¦ã‚¤ã‚¹ã‚­ãƒ¼"],
    "ãƒ¯ã‚¤ãƒ³": ["èµ¤ãƒ¯ã‚¤ãƒ³", "ç™½ãƒ¯ã‚¤ãƒ³", "ãƒ­ã‚¼ãƒ¯ã‚¤ãƒ³", "ã‚¹ãƒ‘ãƒ¼ã‚¯ãƒªãƒ³ã‚°ãƒ¯ã‚¤ãƒ³", "ã‚·ãƒ£ãƒ³ãƒ‘ãƒ³"], "æ³¡": ["ã‚¹ãƒ‘ãƒ¼ã‚¯ãƒªãƒ³ã‚°ãƒ¯ã‚¤ãƒ³", "ã‚·ãƒ£ãƒ³ãƒ‘ãƒ³"],
    "ãƒ“ãƒ¼ãƒ«": ["ãƒ“ãƒ¼ãƒ«", "æµ·å¤–ãƒ“ãƒ¼ãƒ«", "åœ°ãƒ“ãƒ¼ãƒ«ãƒ»ã‚¯ãƒ©ãƒ•ãƒˆãƒ“ãƒ¼ãƒ«"],
    "ã‚µãƒ¯ãƒ¼": ["ã‚µãƒ¯ãƒ¼ã®ç´ ãƒ»å‰²æ", "ãƒªã‚­ãƒ¥ãƒ¼ãƒ«"],
    "æ—¥æœ¬é…’": ["æ—¥æœ¬é…’"],
}

# --- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ (é ‘ä¸ˆç‰ˆ) ---
@st.cache_resource
def load_all_models():
    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿ (å¿…é ˆ)
    try:
        with open('sake_database.pkl', 'rb') as f:
            db_data = pickle.load(f)
    except FileNotFoundError:
        st.error("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹(sake_database.pkl)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    # 2. CLIPãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ (å¿…é ˆãƒ»è‡ªå‹•DL)
    try:
        clip_model = SentenceTransformer('sentence-transformers/clip-ViT-B-32-multilingual-v1')
        all_vectors = np.concatenate([item['vector'] for item in db_data], axis=0)
    except Exception as e:
        st.error(f"CLIPãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    
    # ã‚¸ãƒ£ãƒ³ãƒ«ã‚½ãƒ¼ãƒˆ
    raw_genres = list(set([item.get('genre', 'ãã®ä»–') for item in db_data]))
    sorted_genres = sorted(raw_genres, key=lambda x: GENRE_ORDER.index(x) if x in GENRE_ORDER else 999)

    # 3. ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ (ä»»æ„: ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—)
    intent_tk, intent_md, genre_tk, genre_md = None, None, None, None
    has_logic_model = False

    try:
        intent_path = "./my_intent_model"
        genre_path = "./my_genre_model"
        
        if os.path.exists(intent_path) and os.path.exists(genre_path):
            intent_tk = BertTokenizer.from_pretrained(intent_path)
            intent_md = BertForSequenceClassification.from_pretrained(intent_path)
            genre_tk = BertTokenizer.from_pretrained(genre_path)
            genre_md = BertForSequenceClassification.from_pretrained(genre_path)
            has_logic_model = True
    except Exception:
        pass # ãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã¯ç„¡è¦–ã—ã¦é€²ã‚€

    return {
        "db": db_data,
        "clip": clip_model,
        "vectors": all_vectors,
        "genres": sorted_genres,
        "intent_tk": intent_tk,
        "intent_md": intent_md,
        "genre_tk": genre_tk,
        "genre_md": genre_md,
        "has_logic_model": has_logic_model
    }

models = load_all_models()
if not models:
    st.stop()

# --- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é–¢æ•° ---

# Intent / Genre æ¨è«–
def predict_intent(text):
    if not models["has_logic_model"]: return False, 0.0
    inputs = models["intent_tk"](text, return_tensors="pt", padding=True, truncation=True, max_length=64)
    with torch.no_grad(): outputs = models["intent_md"](**inputs)
    probs = F.softmax(outputs.logits, dim=-1)
    return probs[0][1].item() > 0.5, probs[0][1].item()

def predict_genre_probs(text):
    if not models["has_logic_model"]: return {}
    inputs = models["genre_tk"](text, return_tensors="pt", padding=True, truncation=True, max_length=64)
    with torch.no_grad(): outputs = models["genre_md"](**inputs)
    probs = F.softmax(outputs.logits, dim=-1)[0]
    return {models["genre_md"].config.id2label[i]: prob.item() for i, prob in enumerate(probs)}

# MMR (Maximal Marginal Relevance) ä¸¦ã³æ›¿ãˆãƒ­ã‚¸ãƒƒã‚¯
def mmr_sort(query_vec, candidate_vectors, candidate_items, top_k=12, diversity=0.4):
    """
    diversity: 0ã«è¿‘ã„ã»ã©é¡ä¼¼åº¦é‡è¦–ï¼ˆä»Šã¾ã§é€šã‚Šï¼‰ã€1ã«è¿‘ã„ã»ã©å¤šæ§˜æ€§é‡è¦–ï¼ˆãƒãƒ©ãƒãƒ©ã«ãªã‚‹ï¼‰
    æ¨å¥¨å€¤: 0.3 ~ 0.5
    """
    # ã‚¯ã‚¨ãƒªã¨ã®é¡ä¼¼åº¦è¨ˆç®—
    # candidate_vectorsã¯numpyé…åˆ—ãªã®ã§torch tensorã«å¤‰æ›ã—ã¦è¨ˆç®—
    cand_tensor = torch.tensor(candidate_vectors)
    query_tensor = torch.tensor(query_vec).unsqueeze(0)
    
    sims_to_query = util.cos_sim(query_tensor, cand_tensor)[0]
    
    # MMRã«ã‚ˆã‚‹é¸æŠœãƒ«ãƒ¼ãƒ—
    selected_indices = []
    candidate_indices = list(range(len(candidate_items)))
    
    # å€™è£œãŒå°‘ãªã™ãã‚‹å ´åˆã¯å˜ç´”ã‚½ãƒ¼ãƒˆã§è¿”ã™
    if len(candidate_items) <= top_k:
        sorted_indices = torch.argsort(sims_to_query, descending=True).tolist()
        return [candidate_items[i] for i in sorted_indices], [sims_to_query[i].item() for i in sorted_indices]

    for _ in range(min(len(candidate_items), top_k)):
        best_mmr_score = -float('inf')
        best_idx = -1
        
        for idx in candidate_indices:
            # ã‚¯ã‚¨ãƒªã¨ã®é¡ä¼¼åº¦
            similarity_to_query = sims_to_query[idx].item()
            
            # ã™ã§ã«é¸ã‚“ã ã‚‚ã®ã¨ã®é¡ä¼¼åº¦ï¼ˆæœ€å¤§å€¤ï¼‰
            if selected_indices:
                # é¸ã°ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                selected_vecs = cand_tensor[selected_indices]
                current_vec = cand_tensor[idx].unsqueeze(0)
                sim_to_selected = util.cos_sim(current_vec, selected_vecs)
                max_similarity_to_selected = torch.max(sim_to_selected).item()
            else:
                max_similarity_to_selected = 0
            
            # MMRã‚¹ã‚³ã‚¢ = (1-Î»)*ã‚¯ã‚¨ãƒªé¡ä¼¼åº¦ - Î»*é¸å®šæ¸ˆã¿ã¨ã®é¡ä¼¼åº¦
            mmr_score = (1 - diversity) * similarity_to_query - diversity * max_similarity_to_selected
            
            if mmr_score > best_mmr_score:
                best_mmr_score = mmr_score
                best_idx = idx
        
        selected_indices.append(best_idx)
        candidate_indices.remove(best_idx)
    
    # çµæœã®æ§‹ç¯‰
    results = [candidate_items[i] for i in selected_indices]
    result_scores = [sims_to_query[i].item() for i in selected_indices]
    
    return results, result_scores

# --- æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³æœ¬ä½“ ---
def search_engine(original_query, selected_genres, min_p, max_p, mode="visual", logic_mode="A"):
    ai_message = ""
    search_genres = []
    
    # â˜… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° (æ¤œè¨¼ç”¨ C/Dãƒ¢ãƒ¼ãƒ‰)
    # ã‚¯ã‚¨ãƒªã‚’åŠ å·¥ã—ã¦ã€ŒãŠé…’ã®è¦‹ãŸç›®ã‚’æ¢ã—ã¦ã„ã‚‹ã€ã“ã¨ã‚’å¼·èª¿ã™ã‚‹
    if mode == "visual" and ("C" in logic_mode or "D" in logic_mode):
        # æ—¥æœ¬èªã¨è‹±èªã‚’æ··ãœã¦CLIPã«ä¼ã‚ã‚Šã‚„ã™ãã™ã‚‹ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯
        query_for_clip = f"ã€Œ{original_query}ã€ã¨ã„ã†é›°å›²æ°—ã®ã€ãŠé…’ã®ãƒœãƒˆãƒ«ãƒ‡ã‚¶ã‚¤ãƒ³ã€‚ Package design of sake bottle with the vibe of {original_query}."
        if DEBUG_MODE: st.toast(f"Modified Query: {query_for_clip}")
    else:
        query_for_clip = original_query

    # ã‚¸ãƒ£ãƒ³ãƒ«çµã‚Šè¾¼ã¿ãƒ­ã‚¸ãƒƒã‚¯
    if selected_genres:
        search_genres = selected_genres
    elif mode == "logic" and models["has_logic_model"]:
        # (æ—¢å­˜ã®Logicãƒ¢ãƒ¼ãƒ‰å‡¦ç†...)
        target_genres = []
        for broad_key, children in BROAD_CATEGORIES.items():
            if broad_key in original_query: target_genres.extend(children)
        for g in models["genres"]:
            if g in original_query and g not in target_genres: target_genres.append(g)
        
        if target_genres:
            search_genres = list(set(target_genres))
            ai_message = "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ã‚¸ãƒ£ãƒ³ãƒ«ã‚’çµã‚Šè¾¼ã¿ã¾ã—ãŸ"
        else:
            is_nonal, nonal_conf = predict_intent(original_query)
            if is_nonal:
                search_genres = ["ãƒãƒ³ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«"]
                ai_message = "ãƒãƒ³ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«å•†å“ã‹ã‚‰æ¢ã—ã¾ã™"
            else:
                genre_probs = predict_genre_probs(original_query)
                sorted_genres = sorted(genre_probs.items(), key=lambda x: x[1], reverse=True)
                candidates = [sorted_genres[0][0]]
                for g, p in sorted_genres[1:]:
                    if p > 0.15: candidates.append(g)
                search_genres = candidates
                ai_message = f"AIæ¨è«–: {search_genres[0]} ãªã©ãŒåˆã„ãã†ã§ã™"

    elif mode == "visual" or not models["has_logic_model"]:
        search_genres = [] 
        ai_message = ""

    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    query_vec = models["clip"].encode(query_for_clip, convert_to_tensor=True).cpu().numpy()
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    valid_indices = []
    for i, item in enumerate(models["db"]):
        if search_genres and item.get('genre') not in search_genres: continue
        if not (min_p <= item['price'] <= max_p): continue
        valid_indices.append(i)
        
    if not valid_indices: return [], ai_message
    
    # å€™è£œãƒ‡ãƒ¼ã‚¿ã®ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡º
    target_vectors = models["vectors"][valid_indices]
    candidate_items = [models["db"][i] for i in valid_indices]

    # â˜… ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯åˆ†å² (æ¤œè¨¼ç”¨ B/Dãƒ¢ãƒ¼ãƒ‰)
    if mode == "visual" and ("B" in logic_mode or "D" in logic_mode):
        # MMR (å¤šæ§˜æ€§é‡è¦–)
        # diversity=0.4 ãã‚‰ã„ãŒãƒãƒ©ãƒ³ã‚¹è‰¯ã—
        results, raw_scores = mmr_sort(query_vec, target_vectors, candidate_items, top_k=12, diversity=0.4)
    else:
        # æ—¢å­˜ (å˜ç´”ãªé¡ä¼¼åº¦é †)
        scores = util.cos_sim(query_vec, target_vectors)[0]
        sorted_args = torch.argsort(scores, descending=True)
        
        results = []
        raw_scores = []
        for i in range(min(12, len(sorted_args))):
            idx = sorted_args[i].item()
            results.append(candidate_items[idx])
            raw_scores.append(scores[idx].item())

    # çµæœãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢ (ã‚¹ã‚³ã‚¢ä»˜ä¸ãªã©)
    final_results = []
    for item, raw_score in zip(results, raw_scores):
        # visualãƒ¢ãƒ¼ãƒ‰ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ãªã—ãªã‚‰ä¿‚æ•°é«˜ã‚
        display_score = min(raw_score * 5.0, 0.99) if (mode == "visual" or not models["has_logic_model"]) else min(raw_score * 3.5, 0.99)
        item['match_score'] = display_score
        final_results.append(item)
        
    return final_results, ai_message

# --- UIæ§‹ç¯‰ ---
st.title(f"ğŸ¾ {APP_TITLE}")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
st.sidebar.header("Search Mode")

if models["has_logic_model"]:
    mode_options = ("ã‚¸ãƒ£ã‚±è²·ã„ (æ„Ÿæ€§)", "AIã‚½ãƒ ãƒªã‚¨ (çŸ¥è­˜)")
else:
    mode_options = ("ã‚¸ãƒ£ã‚±è²·ã„ (æ„Ÿæ€§)",) 

mode_select = st.sidebar.radio("æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰", mode_options, index=0)
mode_key = "visual" if "ã‚¸ãƒ£ã‚±è²·ã„" in mode_select else "logic"

st.sidebar.divider()
st.sidebar.header("Filters")
user_genres = st.sidebar.multiselect("ã‚¸ãƒ£ãƒ³ãƒ«å›ºå®š", options=models["genres"])
price_range = st.sidebar.slider("ä¾¡æ ¼å¸¯", 0, 30000, (0, 30000), 500, format="Â¥%d")

# â˜…â˜…â˜… æ¤œè¨¼ç”¨ãƒ¡ãƒ‹ãƒ¥ãƒ¼ (ã“ã“ã‚’è¿½åŠ ï¼) â˜…â˜…â˜…
st.sidebar.divider()
st.sidebar.markdown("### ğŸ§ª é–‹ç™ºè€…ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
logic_mode = st.sidebar.selectbox(
    "æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¤œè¨¼",
    [
        "A: é€šå¸¸ (Baseline)",
        "B: MMR (å¤šæ§˜æ€§é‡è¦–)",
        "C: Prompt (è¨€è‘‰ã‚’è£œæ­£)",
        "D: MMR + Prompt (æœ€å¼·?)"
    ],
    index=0
)
# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…

if DEBUG_MODE: st.sidebar.warning("ğŸ”§ ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ ON")

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
col1, col2 = st.columns([3, 1], vertical_alignment="bottom")
with col1:
    placeholder = "ä¾‹ï¼šã‚µã‚¤ãƒãƒ¼ãƒ‘ãƒ³ã‚¯ãªå¤œã€æ£®ã®ä¸­ã§èª­æ›¸ã€åˆæ‹ã®å‘³..." if mode_key == "visual" else "ä¾‹ï¼šé­šæ–™ç†ã«åˆã†ãƒ¯ã‚¤ãƒ³ã€BBQ..."
    query = st.text_input("ã©ã‚“ãªé›°å›²æ°—ã®ãŠé…’ãŒã„ã„ï¼Ÿ", placeholder=placeholder).strip()
with col2:
    search_btn = st.button("Digã‚‹", type="primary", use_container_width=True)

if query or search_btn:
    if search_btn:
        # GAé€ä¿¡ (A/Bãƒ†ã‚¹ãƒˆç”¨ã«Logicãƒ¢ãƒ¼ãƒ‰ã‚‚ä¸€ç·’ã«é€ã‚‹ã¨åˆ†æã—ã‚„ã™ã„ã‹ã‚‚)
        components.html(f"<script>gtag('event', 'search', {{'search_term': '{query}', 'logic_mode': '{logic_mode}'}});</script>", height=0)

    st.divider()
    
    # æ¤œç´¢å®Ÿè¡Œ (logic_modeã‚’æ¸¡ã™)
    results, message = search_engine(query, user_genres, price_range[0], price_range[1], mode=mode_key, logic_mode=logic_mode)
    
    if message: st.caption(message)
    
    if results:
        cols_count = 3 if mode_key == "visual" else 4
        cols = st.columns(cols_count)
        
        for i, item in enumerate(results):
            with cols[i % cols_count]:
                with st.container(height=450, border=True): 
                    if item.get('image_url'): st.image(item['image_url'], use_container_width=True)
                    else: st.text("No Image")
                    
                    match_percent = int(item['match_score'] * 100)
                    price_str = f"Â¥{item['price']:,}"
                    
                    if mode_key == "visual":
                        st.progress(match_percent / 100, text=f"Match: {match_percent}%")
                        short_name = item['name'] if len(item['name']) < 35 else item['name'][:34] + "â€¦"
                        st.write(f"**{short_name}**")
                        st.caption(f"{item.get('genre')} | {price_str}")
                    else:
                        st.caption(f"ğŸ· {item.get('genre')}")
                        short_name = item['name'] if len(item['name']) < 25 else item['name'][:24] + "â€¦"
                        st.write(f"**{short_name}**")
                        st.write(f"{price_str}")
                        if DEBUG_MODE: st.caption(f"Score: {match_percent}%")

                    st.link_button("æ¥½å¤©ã§è¦‹ã‚‹ â¤", item['url'], use_container_width=True)
    else:
        st.warning("Not found... æ¡ä»¶ã‚’å¤‰ãˆã¦Digã‚Šç›´ã—ã¦ãã ã•ã„ğŸ’¿")